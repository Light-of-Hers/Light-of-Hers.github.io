<!DOCTYPE html><html lang="en" theme-mode="light"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>2023年你最喜欢的MLSys相关的工作是什么（回答） | Renze Chen (陈仁泽)</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="preload" as="font" crossorigin="anonymous" href="/font/Bender.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/BenderLight.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/JetBrainsMono-Regular.woff2"><link rel="stylesheet" href="/css/arknights.css"><style>@font-face {
  font-family: Bender;
  src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
  font-family: BenderLight;
  src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
  font-family: 'JetBrains Mono';
  src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}
@font-face {
  font-family: 'Fira Code';
  src: local('Fira Code'), url('/font/FiraCode-Regular.ttf');
}
@font-face {
  font-family: 'Monaco';
  src: local('Monaco'), url('/font/Monaco.ttf');
}
</style><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"Enter here","blurHolder":"Search","noResult":"Data \"$0\" not found"},"code":{"codeInfo":"$0 - $1 lines","copy":"copy"}}</script><link type="text/css" rel="stylesheet" href="/lib/encrypt/hbe.style.css"><script src="//unpkg.com/mermaid@10.5.0/dist/mermaid.min.js"></script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.woff2") format('woff2');
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><style>:root {
 --dark-background: url('/imgs/bk-dark-0.png');
 --light-background: url('/imgs/bk-light-5.jpg');
 --theme-encrypt-confirm: 'confirm'
}</style><script defer src="/js/arknights.js"></script><script defer src="/js/search.js"></script><script defer type="module">import mermaid from '//unpkg.com/mermaid@10.5.0/dist/mermaid.esm.mjs';
window.mermaid = mermaid;
code.paintMermaid();
</script><script async src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script async src="/lib/encrypt/hbe.js"></script><script async src="/js/pjax.js"></script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.busuanzi'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="Search" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup" tabindex="0"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">About</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Blogs</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>2023年你最喜欢的MLSys相关的工作是什么（回答）</h1><div id="post-info"><span>Post Date: <div class="control"><time datetime="2024-01-05T17:32:00.000Z" id="date"> 2024-01-06</time></div></span><br><span>Blog Link: <div class="control"> <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/637480969/answer/3351537055">[site]</a></div></span></div></div><hr><div id="post-content"><p>2023 年 mlsys 相关的研究中，最有代表性的主题无疑是 llm inference，涌现了不少优秀的工作。llm training 和之前的 transformer-based model training 主要是 scale 上有区别，基本的分布式训练方法大同小异，感觉前两年也大致卷完了。但 llm inference 除了 scale 上的区别使得内存压力和计算压力增大以外，其<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=%E8%87%AA%E5%9B%9E%E5%BD%92%E8%A7%A3%E7%A0%81&zhida_source=entity">自回归解码</a>的性质也使得推理优化变得更加有趣。感觉喜欢的工作挺多的，就都简单列一下吧。</p>
<p>首先是 <strong>Continuous Batching</strong>，出处是 OSDI’22 的工作 <a href="https://link.zhihu.com/?target=https://www.usenix.org/conference/osdi22/presentation/yu">Orca</a> ，不过 “continuous batching” 这个名称还有大家比较熟悉的那个示意图出处好像是 2023 年的一个博客 <a href="https://link.zhihu.com/?target=https://www.anyscale.com/blog/continuous-batching-llm-inference">How continuous batching enables 23x throughput in LLM inference while reducing p50 latency</a> ，而且其大规模应用也在 2023 年，就勉强当作 2023 年的工作吧。主要贡献是通过细粒度的 batching 来改善了 llm serving 过程中硬件利用率问题。</p>
<p class='item-img' data-src='https://picx.zhimg.com/80/v2-9718c8c32354b9797d4f4e24f4900dd0_1440w.webp?source=2c26e567'><img src="https://picx.zhimg.com/80/v2-9718c8c32354b9797d4f4e24f4900dd0_1440w.webp?source=2c26e567"></p>
<p>其次是 <strong>vLLM</strong> (<a href="https://link.zhihu.com/?target=https://dl.acm.org/doi/10.1145/3600006.3613165">SOSP’23 paper</a>, <a href="https://link.zhihu.com/?target=https://github.com/vllm-project/vllm">code</a>)，是 Ion Stoica 门下 <a href="https://link.zhihu.com/?target=https://lmsys.org/">LLMSYS</a> 小组的工作（之前搞出了 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=vicuna&zhida_source=entity">vicuna</a> 和 fast-chat 的组），主要贡献是提出了 paged-attention（类似操作系统中虚拟分页的内存管理机制，以及配套的针对不连续 sequence 的 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=attention+kernel&zhida_source=entity">attention kernel</a>），缓解了 llm decode 阶段动态增长的 kv-cache 造成的内存碎片问题（<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=huggingface&zhida_source=entity">huggingface</a> 采用 concat 来增长 kv-cache，拷贝开销大，外部碎片多；faster-transformer 为每个 kv-cache 预分配固定内存，内部碎片大），由此也增大了 serving 过程的 batching 容量上限，大大提高了吞吐。</p>
<p>然后是 <strong>Speculative Decoding</strong> (<a href="https://link.zhihu.com/?target=https://proceedings.mlr.press/v202/leviathan23a.html">ICML’23 paper</a>)，Google 的工作，借助一个更小的 draft model 来快速生成多个 token，由原始 model 来一次性验证这些 token，相当于变相突破了 llm decode 阶段每次只生成单个 token 的限制，以一些冗余计算为代价提高了硬件利用率，从而提升性能。<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=CMU&zhida_source=entity">CMU</a> 的 Catalyst 组的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=%E8%B4%BE%E5%BF%97%E8%B1%AA&zhida_source=entity">贾志豪</a>&amp;苗旭鹏的工作 <strong>SpecInfer</strong> (<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2305.09781">paper</a>, <a href="https://link.zhihu.com/?target=https://github.com/flexflow/FlexFlow/tree/inference">code</a>) 以 boost 方式训练ensemble draft model，推理时将多个 draft model 的输出组织成 token tree，用 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=tree+attention&zhida_source=entity">tree attention</a> 进行快速验证。还有 <a href="https://link.zhihu.com/?target=https://lmsys.org/">LLMSYS</a> 组的 <strong>Lookahead Decoding</strong> (<a href="https://link.zhihu.com/?target=https://lmsys.org/blog/2023-11-21-lookahead-decoding/">blog</a>, <a href="https://link.zhihu.com/?target=https://github.com/hao-ai-lab/LookaheadDecoding">code</a>) 则是借助并行采样&amp;验证来加速 token 生成，性质上类似于即时”训练”一个 n-gram draft model 的 speculative decoding，同样也是用冗余计算来换取硬件效率，不过因为没借助预先训练的 draft model，上限会略低一些。</p>
<p>最后是 <strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=Flash-Decoding&zhida_source=entity">Flash-Decoding</a></strong> (<a href="https://link.zhihu.com/?target=https://pytorch.org/blog/flash-decoding/">blog</a>, <a href="https://link.zhihu.com/?target=https://github.com/Dao-AILab/flash-attention/tree/v2.4.2">code</a>) 和 <strong>Flash-Decoding++</strong> (<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2311.01282">paper</a>)，前者是 Tri Dao 组的，相当于 Flash-Attention-v3，后者则出自清华汪玉组、上交<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=%E6%88%B4%E5%9B%BD%E6%B5%A9&zhida_source=entity">戴国浩</a>组、初创公司”无问芯穹”(Infinigence-AI)。两者解决的问题基本都是在 batch 不够大的情况下，decode 阶段 attention 的 query-seq-len=1 导致的并行性低下的问题，解决的共同思路都是想办法将 key-value-seq 维度给并行化。flash-decoding 将 flash-attention-v2 的思路和常见的 parallel reduction 结合。而 flash-decoding++ 的思路则更有趣，通过适当选取一个 unified max value（具体 analysis 参见 paper），将 softmax 所施加的 key-value-seq 维度上的 reduce 依赖部分破除，效率会比 flash-decoding 的 parallel reduction 更高。此外，Luis Ceze 门下的 <strong>FlashInfer</strong> (<a href="https://link.zhihu.com/?target=https://sampl.cs.washington.edu/projects/flashinfer.html">project</a>, <a href="https://link.zhihu.com/?target=https://github.com/flashinfer-ai/flashinfer">code</a>) 也为 llm inference 提供了非常高效的 kernel 实现。</p>
<p>上述的工作大多和 llm decode 阶段的特性有关。还有一些通过“压缩”模型来减少内存压力和计算压力的工作感觉也不错，比如几个借助动态稀疏性来推进 llm 本地化部署的工作：Tri Dao 组的 <strong>Deja Vu</strong> (<a href="https://link.zhihu.com/?target=https://proceedings.mlr.press/v202/liu23am.html">ICML’23 paper</a>, <a href="https://link.zhihu.com/?target=https://github.com/FMInference/DejaVu">code</a>)，上交 IPADS 的 <strong>PowerInfer</strong> (<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2312.12456">paper</a>, <a href="https://link.zhihu.com/?target=https://github.com/SJTU-IPADS/PowerInfer">code</a>)，苹果的 <strong>LLM in a Flash</strong> (<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2312.11514">paper</a>)；还有 llm low-bit 动态量化的 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=639447829&content_type=Answer&match_order=1&q=state-of-the-art&zhida_source=entity">state-of-the-art</a>，Luis Ceze 门下的工作 <strong>Atom</strong> (<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2310.19102">paper</a>, <a href="https://link.zhihu.com/?target=https://github.com/efeslab/Atom">code</a>)。</p>
<p>最后附上上述提到的部分工作关于各种系统要素的权衡：</p>
<p class='item-img' data-src='https://pica.zhimg.com/80/v2-c7dd9f5549aa166912d0aef18dbb9d72_1440w.webp?source=2c26e567'><img src="https://pica.zhimg.com/80/v2-c7dd9f5549aa166912d0aef18dbb9d72_1440w.webp?source=2c26e567"></p>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2024/01/26/%E7%9B%AE%E5%89%8D%E6%98%AF%E5%90%A6%E6%9C%89%E6%8C%91%E6%88%98%20Transformer%20%E7%9A%84%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%EF%BC%9F%EF%BC%88%E5%9B%9E%E7%AD%94%EF%BC%89/">← Next 目前是否有挑战 Transformer 的新型架构？（回答）</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2023/09/28/Stable-Diffusion%20+%20ControlNet%20%E7%9A%84%20UNet%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%89%96%E6%9E%90/">Stable-Diffusion + ControlNet 的 UNet 网络结构剖析 Prev →</a></div></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="To Top" style="opacity: 0; display: none;">∧ </a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="Change Theme"></a></div></div></article><aside><div id="about"><a target="_blank" rel="noopener" href="https://prts.wiki/w/%E6%96%87%E4%BB%B6:%E6%A8%A1%E7%BB%84_%E5%8C%BB%E4%B8%8D%E8%87%AA%E6%B2%BB.png" id="logo"><img src="/imgs/avatar.png" alt="Logo"></a><h1 id="Dr"><a href="/">Renze Chen</a></h1><div id="description"><p>陈仁泽</p></div><div id="social-links"><a class="social" target="_blank" rel="noopener" href="https://github.com/Light-of-Hers"><i class="fab fa-github" alt="GitHub"></i></a><a class="social" target="_blank" rel="noopener" href="https://www.zhihu.com/people/yi-guang-99-48"><i class="fab fa-zhihu" alt="Zhihu"></i></a><a class="social" target="_blank" rel="noopener" href="https://orcid.org/0000-0001-5938-7965"><i class="fab fa-orcid" alt="ORCID"></i></a><a class="social" target="_blank" rel="noopener" href="https://dblp.org/pid/260/5910"><i class="iconfont icon-dblp" alt="DBLP"></i></a><a class="social" href="mailto:crz@pku.edu.cn"><i class="fa fa-envelope" alt="E-Mail"></i></a></div></div><div id="aside-block"></div><footer><nobr>Published with <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> Theme <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> by <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main></body></html>